{"cells":[{"metadata":{"_uuid":"05ff059b35595eae90e8d8642df2be40865e400a"},"cell_type":"markdown","source":"# Introduction\n\nIn this tutorial, I will show logistic regression algorithms. Step by step I will show you what mathematical formulas we need to use. I'll show you how to do this using the sklearn library and how to do it without it.\n\nLogistic Regression algorithm is used to predict if the result is Yes or No, A or B, Cat or Dog etc. Our data set includes back pain symptoms that are classified as Abnormal or Normal and suitable for applying Logistic Regression.\n\nLogistic regression is the most famous machine learning algorithm after linear regression. In a lot of ways, linear regression and logistic regression are similar. But, the biggest difference lies in what they are used for. Linear regression algorithms are used to predict/forecast values but logistic regression is used for classification tasks.\n\nProstate cancer is a disease in which malignant (cancer) cells form in the tissues of the prostate.\n\nOur data set contains prostate cells that are classified as good or bad and are appropriate for the use of logistic regression.\nfirst, let's import the necessary libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a79a58d43f040e85232ee90a7ed45213bda4d3a7"},"cell_type":"markdown","source":"**Exploratory Data Analysis**\n\nFirst, we need to explore our data set. in this, we need some statistical data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/Prostate_Cancer.csv')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13846fab113f756982b789ee01437030baa9816e"},"cell_type":"markdown","source":"There are 10 columns and 100 rows in total. There is no missing value. There are 1 types of objects, while others are INT and float types. I will convert all columns to float type in order to make it more convenient. I'il erase your id value."},{"metadata":{"trusted":true,"_uuid":"e7aaa20e921309bda6e970fb044874bca7c1d76d"},"cell_type":"code","source":"df.drop([\"id\"],axis=1,inplace = True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94124957ae6bcbeeff4b129032a7f5ff09d68252"},"cell_type":"code","source":"# we convert data type the float.\ncolumns = ['radius','texture','perimeter','area']\n\nfor column in columns:\n    df[column] = df[column].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e05330524468210979c71c2a4ba0b8db915e06d"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb3f50dd558fa29651bdf03f53532f9f874f5758"},"cell_type":"markdown","source":"Let's look at the data for the first 15."},{"metadata":{"trusted":true,"_uuid":"78375875d81f121a96843a84a7fd372e4fa6ba14"},"cell_type":"code","source":"df.head(15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"526c51db119d1f0df610a7dfbbfb5417f41bbf1e"},"cell_type":"markdown","source":"diagnosis_result column includes only two class names; M and B. Let's convert it as 1 and 0."},{"metadata":{"trusted":true,"_uuid":"3740a5ddc7aa361a8c1126e471fdeb543765c7fb"},"cell_type":"code","source":"#Convert two class names to 0 and 1\ndf.diagnosis_result = [1 if each == \"M\" else 0 for each in df.diagnosis_result]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb9b8735f62f0e2992914fc2860a76d46eeead1c"},"cell_type":"markdown","source":"describe is gives some statistical values about data set, such as max,min,mean,median etc."},{"metadata":{"trusted":true,"_uuid":"90089400e89e2e8ae60bb40b4983fcaaff342e8b"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"283fe2bf118463a8f6801b642bad19dd3f485a6a"},"cell_type":"markdown","source":"**Assigning X and Y values**\n\nWe need to split our data into two dimension as x and y values. y value is our class name, which gives if the row is M or B. So, we assign diagnosis_result column as y values then drop the column from data set. The rest of the data ; all radius, texture..., fractal_dimension columns will be our x value."},{"metadata":{"trusted":true,"_uuid":"a02e9792ff51c31ebf34bf6fd3e559742a46225a"},"cell_type":"code","source":"#assign Class_att column as y attribute\ny = df.diagnosis_result.values\n\n#drop Class_att column, remain only numerical columns\nx_data = df.drop([\"diagnosis_result\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25ec3d501fac5f461046985acff36803ada9b72a"},"cell_type":"markdown","source":"**Normalization**\n\nStatistical data shows us how big some values are and how small some values are. Here, 1area data is so big that it affects others. For example, 1symetry data is very small, and when we use it to process it will have no effect, because other data is very large. So we need to normalize this. Below is the formula for normalization. Here, the minimum value of each value is subtracted from itself and then the maximum value of each value is subtracted from the minimum value. We're splitting each other.\n\n**Normalization = (x - min(x))/(max(x)-min(x))**"},{"metadata":{"trusted":true,"_uuid":"d5eca82ed7c2617e48d6f7d2a310e89f33fce822"},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38e04a80d1e746d919d323020fd3d3c85a13a65d"},"cell_type":"markdown","source":"**Let's split the data as test and train.**\n\nWe need to determine whether our algorithm is successful. We're going to split the data for this. Where 20% of the data is usually test data and 80% is training data. We'il train our algorithm. At the end of the training we will compare the results with the real data.\n\nThe Train_test_split function randomly divides data. If we give a random_state parameter, it always splits the data in the same way. This prevents you from receiving different results each time you run the entire algorithm. the random_state = 42 parameter is used to stabilize randomization. random_state divides the data randomly, but you receive the same train and test data according to a rule and each time you run the algorithm."},{"metadata":{"trusted":true,"_uuid":"04e77c1cded575bd4ccddda8320669e0917436be"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state =42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9627cdf780a138abf9c411be89775ad80224f8b4"},"cell_type":"markdown","source":"At this point, there are two methods. The first is a long way to apply mathematical formulas and the other is a short way to use the sklearn library. The above code is common to both solutions.\n\nBefore you begin, I will transpose all x_train, x_test, y_train, y_test  matrices to make the matrix multiplication easier for the next steps."},{"metadata":{"trusted":true,"_uuid":"3890c70cdb6d2c9872bd505d2f1c18d56b9b2ed4"},"cell_type":"code","source":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc34008552b73505746063cb5a2a59cf76469cd0"},"cell_type":"markdown","source":"**The First Way**"},{"metadata":{"_uuid":"b35c6f0863d0d6d0c057b2813e2b35df88936b26"},"cell_type":"markdown","source":"![](https://cdn-images-1.medium.com/max/800/1*X-FJtdd1MgyA-h5ovc7X5w.jpeg)"},{"metadata":{"_uuid":"1aae2c5dc0051bb07767551617558fd4a334f077"},"cell_type":"markdown","source":"Most often, we would want to predict our outcomes as YES/NO (1/0).\n\nFor example:\n\nIs your favorite football team going to win the match today? — yes/no (0/1)\n\nDoes a student pass in exam? — yes/no (0/1)\n\nThe logistic function is given by:\n\nf(x) = L/(1+e ^-k(x-x0))\n\nwhere\n\nL – Curve’s maximum value\n\nk – Steepness of the curve\n\nx0 – x value of Sigmoid’s midpoint\n\nA standard logistic function is called sigmoid function (k=1,x0=0,L=1)"},{"metadata":{"trusted":true,"_uuid":"8a5b5eeecf2f63eee41ba6da54a37b2ecfb7618d"},"cell_type":"code","source":"#parameter initialize and sigmoid function\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01) #first initialize w values to 0.01\n    b = 0.0 #first initialize bias value to 0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d5c88f39f0f4caa8bf02cc76de3952e0fcbe0b2"},"cell_type":"markdown","source":"This is sigmoid formula -> S(x) = 1/1+ ( e ^ - x)\n\nThe sigmoid function gives an ‘S’ shaped curve.\n\nThis curve has a finite limit of:\n\n‘0’ as x approaches −∞\n\n‘1’ as x approaches +∞\n\nThe output of sigmoid function when x=0 is 0.5\n\nThus, if the output is more tan 0.5 , we can classify the outcome as 1 (or YES) and if it is less than 0.5 , we can classify it as 0(or NO) ."},{"metadata":{"trusted":true,"_uuid":"f204fe4c3d248b06450fed187478ac06bc44a0c8"},"cell_type":"code","source":"#sigmoid function fits the z value between 0 and 1\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ec68ef87b8dee7eb2029d5d7d0182137d74fdf1"},"cell_type":"markdown","source":"Let's write a function that calculates y_head, loss and cost values as forward propagation, and calculates derivatives of w and b as backpropagation."},{"metadata":{"trusted":true,"_uuid":"53fec27b9d738f4c8c3717861983fbb159b2689c"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    \n    y_head = sigmoid(np.dot(w.T,x_train) + b)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) / x_train.shape[1]\n    \n    # backward propagation\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9394e08fb5afb49700f74cfd19e801e45acdaa99"},"cell_type":"markdown","source":"if the weight and bias values are not as desired, we will update them with the following function."},{"metadata":{"trusted":true,"_uuid":"1b2fa410885ba42d5272850ec56561ff81b670be"},"cell_type":"code","source":"def update_weight_and_bias(w,b,x_train,y_train,learning_rate,iteration_num) :\n    cost_list = []\n    index = []\n    \n    #for each iteration, update w and b values\n    for i in range(iteration_num):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        w = w - learning_rate*gradients[\"derivative_weight\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        \n        cost_list.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": w,\"bias\": b}\n    \n    print(\"iteration_num:\",iteration_num)\n    print(\"cost:\",cost)\n\n    #plot cost versus iteration graph to see how the cost changes over number of iterations\n    plt.plot(index,cost_list)\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd7553501536ff63d5ca790b26b92603ede94062"},"cell_type":"markdown","source":"We're writing our code for the guess. Here we have already obtained the necessary w and b values."},{"metadata":{"trusted":true,"_uuid":"68e7f09080172e54eac6bebffceac0b5af689c7d"},"cell_type":"code","source":"def predict(w,b,x_test):\n    z = np.dot(w.T,x_test) + b\n    y_predicted_head = sigmoid(z)\n    \n    #create new array with the same size of x_test and fill with 0's.\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_predicted_head.shape[1]):\n        if y_predicted_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ddc068fffd6a0383d3cba8ed826c2589dc758f0"},"cell_type":"markdown","source":"We combine all the formulas in a function. For accuracy, we compare the y_test value and the calculated y_prediction array. Both arrays contain values 0 and 1. If y_test and y_prediction are the same, the result is true."},{"metadata":{"trusted":true,"_uuid":"5e7f3a46b3c1390358ce5d5df11b9c660d9fa355"},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,iteration_num):\n    dimension = x_train.shape[0]#For our dataset, dimension is 248\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients = update_weight_and_bias(w,b,x_train,y_train,learning_rate,iteration_num)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    # Print test Accuracy\n    print(\"manuel test accuracy:\",(100 - np.mean(np.abs(y_prediction - y_test))*100)/100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbdfd0ea3b73591c4f4b9190fbdd67bec970963e"},"cell_type":"markdown","source":"we will give some values to learning rate and number_of_iterations parameters. Start with learning_rate = 1 and number_of_iterations = 10"},{"metadata":{"trusted":true,"_uuid":"1b543ec1b751947f7f6b119faba07f08ff041702"},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,1,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aec927a4d9ae3be0ca5083c52c51341525e21f27"},"cell_type":"markdown","source":"Now let's give Learning_rate = 10 and number_of_iterations = 10."},{"metadata":{"trusted":true,"_uuid":"0c01e69bb2a2efb652e0253ef0ec4c8a8abaeba4"},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,10,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7d0dbcaf6dc3d1befd63f9b7660cab01e142840"},"cell_type":"markdown","source":"as learning_rate increases, it increases in cost, but it increases in accuracy."},{"metadata":{"trusted":true,"_uuid":"2e9c6afb424299e3992fde1496401f8a2d0717e3"},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,4,200)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"199122b4641d5ca3c2d8aa2015b229cbb7cc0639"},"cell_type":"markdown","source":"when learning_rate = 4 and iteration_number = 200, the cost has been reduced too. The truth rate is very high."},{"metadata":{"_uuid":"a85efd8a1254512a7ed985c7171b468a5d58b6f8"},"cell_type":"markdown","source":"**The Second Way : Sklearn Library **\n\nWith using sklearn library, we can create a logistic regression model and find the accuracy easliy like below:"},{"metadata":{"trusted":true,"_uuid":"0f7e9b7fcf83c847ffed47416b793382fd753942"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\n\nlr_model.fit(x_train.T,y_train.T)\n\nprint(\"sklearn test accuracy:\", lr_model.score(x_test.T,y_test.T))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6af429904b9f0bafa96e4e0477817e21315385da"},"cell_type":"markdown","source":"# **Conclusion**\n\n* If you like it, thank you for you upvotes.\n* If you have any question, I will happy to hear it"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}